\section{Обзор современных методов и технологий серверной виртуализации} \label{literature}

Понятие облачных вычислений на сегодняшний день уже достаточно хорошо известно и в информационных технологиях (ИТ) и бизнесе.
Облачные вычисления являются новой сервисной моделью предоставления вычислительных услуг.
Еженедельно появляются новые статьи, презентации, книги об облачных вычислениях, что говорит о заинтересованности сообщества в данных технологиях.

За время существования информационных технологий создавались и изменялись подходы к построению информационных систем.
Первой моделью информационной системы была монолитная архитектура.
В данной модели на одном компьютере работали и приложения и база данных (БД), а пользователи сидели у <<тонких>> терминалов которые отображали информацию с компьютера.
У архитектуры было большое количество недостатков, поэтому впоследствии ее сменила более перспективная клиент-серверная архитектура.
В этом случае на компьютере располагался выделенный сервер баз данных, а пользователи с <<толстых клиентов>> разгружали сервер БД.
Затем появилась еще более современная многоуровневая архитектура, у которой логика приложений вынесена на отдельный компьютер, который называется сервер приложений, а пользователи работали на <<тонких>> клиентах через веб-браузеры.
В современном информационном мире большинство приложений выполнено именно в многоуровневой архитектуре.
Она подразумевает развертывание всей ИТ-инфраструктуры на территории заказчика \cite{oracle-db}.

Облачные вычисления являются следующим шагом в эволюции архитектуры построения информационных систем.
Благодаря преимуществам данного подхода вполне очевидно, что многие информационные системы в ближайшее время переносятся или будут перенесены в облако.
Процесс уже идет полным ходом и его игнорирование или недооценка может привести к поражению в конкурентной борьбе на рынке.
В данном случае имеется в виду не только отставание ИТ или неоправданные затраты на него, но и отставание в развитии бизнеса компании, которая зависит от гибкости информационной инфраструктуры и скорости вывода новых сервисов и продуктов на рынок.

В феврале 2011 года ИТ-директор американского правительства Вивек Кундра опубликовал стратегию переноса части информационных систем в облако.
Стратегия была опубликована в документе под названием <<Federal Cloud Computing Strategy>>, который четко описывает порядок и сроки переноса.
Целью работ является уменьшение сложности и повышение управляемости ИТ, увеличение нагрузки оборудования до 70-80\%, а также уменьшение количества центров обработки данных.

Дата-центр, иначе именуемый центром обработки данных, является специализированным зданием, в котором размещается серверное и сетевое оборудование, подключаемое абонентов к сети Интернет.
Основное требование, предъявляемое к центрам обработки данных, отказоустойчивость.
При этом подразумевается отключение дата-центра как на время планово-предупредительных работ и профилактики оборудования, так и внеплановых аварийных ситуаций.

Классификация Tier описывает надежность функционирования ЦОД и является необходимой для компаний, как желающих построить свой ЦОД, так и для арендующих чужие вычислительные мощности.
В зависимости от критичности бизнеса и потерь, которые понесет компания в случае остановки бизнес-процессов, выбирается тот или иной уровень надежности.
В свою очередь, высокий уровень надежности требует высоких материальных и эксплуатационных затрат, поэтому и стоимость вычислительных мощностей зависит от уровня надежности ЦОД \cite{dc-tier}.

На сегодняшний день существует четыре уровня надежности ЦОД названные Tier I, Tier II, Tier III и Tier IV, которые были введены организацией Uptime Institute (Институт бесперебойных процессов, США):
\begin{itemize}
  \item Tier I: время простоя 28.8 часов в год, коэффициент отказоустойчивости 99.671\%;
  \item Tier II: 22.0 часа в год, 99.749\%;
  \item Tier III: 1.6 часа в год, 99.982\%;
  \item Tier IV: 0.4 часа в год, 99.995\%.
\end{itemize}

ЦОД уровня Tier I (базовый уровень) подвержен нарушениями работы как от плановых, так и от внеплановых действий.
Применение фальшпола, источников бесперебойного питания (ИБП), дизель-генераторных установок (ДГУ) не обязательно.
Если ИБП и ДГУ используются, то выбираются более простые модели, без резерва, с множеством точек отказа.
Возможны самопроизвольные отказы оборудования.
К простою ЦОД также приведут ошибки в действиях обслуживающего персонала.
В таких ЦОД отсутствует защита от случайных и намеренных событий, обусловленных действиями человека.

В ЦОД уровня Tier II (с резервированными компонентами) время простоя возможно в связи с плановыми и внеплановыми работами, а также аварийными ситуациями, однако оно сокращено благодаря внедрению одной резервной единицы оборудования в каждой системе.
Таким образом, системы кондиционирования, ИБП и ДГУ имеют одну резервную единицу, тем не менее, профилактические работы требуют отключения ЦОД.
В центрах обработки данных с резервированными компонентами требуется наличие минимальных защитных мер от влияния человека.

Третий уровень надежности (уровень с возможность параллельного проведения ремонтных работ) требует осуществления любой плановой деятельности без остановки ЦОД.
Под плановыми работами подразумевается профилактическое и программируемое техническое обслуживание, ремонт и замена компонентов, добавления или удаление компонентов, а также их тестирование.
В таком случае необходимо иметь резерв, благодаря которому можно пустить всю нагрузку по другому пути во время работ на первом.
Для реализации Tier III необходима схема резервирования блоков схем кондиционирования, ИБП, ДГУ N+1, также требуется наличие двух комплектов трубопроводов для системы кондиционирования, построенной на основе чиллера (холодильной машины).
Строительные требования обязывают сохранять работоспособность ЦОД при большинстве случаев намеренных и случайных вмешательств человека.
Также следует предусмотреть резервные входы, дублирующие подъездные пути, контроль доступа, отсутствие окон, защиту от электромагнитного изучения.

Четвертый уровень надежности ЦОД (отказоустойчивый) характеризуется безостановочной работой при проведении плановых мероприятий и способен выдержать один серьезный отказ без последствий для критически важной нагрузки.
Необходим дублированный подвод питания, резервирование системы кондиционирования и ИБП по схеме 2(N+1).
Для дизель-генераторных установок необходима отдельная площадка с зоной хранения топлива.
Tier IV требует защиту от всех потенциальных проблем в связи с человеческим фактором.
Регламентированы избыточные средства защиты от намеренных или случайных действий человека.
Учтено влияние сейсмоявлений, потопов, пожаров, ураганов, штормов, терроризма.

Дата-центры по виду использования подразделяют на корпоративные и коммерческие (аутсорсинговые).
Корпоративные ДЦ предназначены для обслуживания конкретной компании, коммерческие, в свою очередь, предоставляют услуги всем желающим.

Некоторые ДЦ предлагают клиентам дополнительные услуги по использованию оборудования, по автоматическому уходу от различных видов атак.
Команда специалистов круглосуточно производит мониторинг серверов.
Для обеспечения сохранности данных используются системы резервного копирования.
Для предотвращения кражи данных, в дата-центрах используются различные системы ограничения физического доступа, системы видеонаблюдения.

Дата-центры предоставляют несколько основных типов услуг, среди которых:
\begin{itemize}
  \item виртуальный хостинг (shared hosting);
  \item виртуальный сервер (virtual private/dedicated server);
  \item выделенный сервер (dedicated server);
  \item размещение сервера (colocation);
  \item выделенная зона (dedicated area).
\end{itemize}

Виртуальный хостинг используется для размещения большого количества сайтов на одном веб-сервере.
В основном для построения веб-сервера используется типичный стек технологий LAMP, где в качестве операционной системы выступает GNU/Linux, http-сервер Apache (зачастую в связке с nginx), сервер баз данных MySQL, интерпретируемые скриптовые языки PHP, Perl, Python.
Существует решение на базе ОС Windows Server, где в качестве http-сервера используется IIS, в качестве СУБД выступает MS SQL, а также существует поддержка платформы ASP.NET.
Разделение ресурсов на виртуальном хостинге основывается на ограничении дискового пространства, сетевого трафика, количества используемых доменов, почтовых ящиков, баз данных, FTP-аккаунтов, ограничение на использование процессорного времени, памяти для PHP-скриптов и так далее.

Виртуальный выделенный сервер эмулирует работу отдельного физического сервера.
На одной физической машине может быть запущено несколько виртуальных серверов, при этом каждый виртуальный сервер имеет свои процессы, ресурсы и отдельное администрирование.
Для реализации виртуальных машин используются технологии виртуализации, как системы с открытым исходным кодом, так и коммерческие.

В случае выделенного сервера клиенту целиком предоставляется отдельная физическая машина.
Владелец сервера имеет возможность смены конфигурации оборудования, установки любой операционной системы.
Такой тип хостинга подходит для высоконагруженных проектов.

Размещение сервера отличается от услуги предоставления выделенного сервера тем, что ДЦ размещает у себя сервер, который заранее подготовил клиент.
Дата-центр подключает его в общую инфраструктуру ЦОДа, обеспечивает бесперебойное электропитание, охлаждение, доступ к сетевому каналу, удаленный доступ к серверу, охрану, мониторинг и другие услуги.

Выделенная зона предоставляется в основном для специальных клиентов, имеющих строгие нормы безопасности.
В этом случае дата-центр предоставляет выделенную зону, обеспеченную электроснабжением, холодоснабжением и системами безопасности, а клиент сам создает свой дата-центр внутри этого пространства.

Также можно выделить такую услугу, как аренда телекоммуникационных стоек, которая является частным случаем размещения сервера, с отличием в том, что арендаторами в основном являются юридические лица.

При построении облачной инфраструктуры важную роль играет виртуализация.

Виртуализация --- абстракция вычислительных ресурсов и предоставление пользователю системы, которая инкапсулирует (скрывает в себе) собственную реализацию.
Проще говоря, пользователь работает с удобным для себя представлением объекта, и для него не имеет значения, как объект устроен в действительности.
Термин <<виртуализация>> появился в шестидесятых годах XX века, а в девяностых --- стали очевидны перспективы подхода: с ростом аппаратных мощностей, как персональных компьютеров, так и серверных решений, вскоре представится возможность использовать несколько виртуальных машин на одной физической платформе.

Понятие виртуализации можно условно разделить на две категории \cite{ibm-virt}:
\begin{itemize}
  \item виртуализация платформ, продуктом этого вида виртуализации являются виртуальные машины --- некие программные абстракции, запускаемые на платформе реально аппаратно-программных систем;
  \item виртуализация ресурсов преследует целью комбинирование или упрощение представления аппаратных ресурсов для пользователя и получение неких пользовательских абстракций оборудования, пространств имен, сетей.
\end{itemize}

Когда производится виртуализация, существует множество способов ее осуществления.
Фактически есть несколько путей, с помощью которых достигаются одинаковые результаты через разные уровни абстракции \cite{openvz-tutorial}:
\begin{itemize}
  \item эмуляция аппаратных средств;
  \item полная виртуализация;
  \item паравиртуализация;
  \item виртуализация уровня ОС.
\end{itemize}

Эмуляция аппаратных средств является одним из самых сложных методов виртуализации.
В то же время главной проблемой при эмуляции аппаратных средств является низкая скорость работы, в связи с тем, что каждая команда моделируется на основных аппаратных средствах.
В эмуляции оборудования используется механизм динамической трансляции, то есть каждая из инструкций эмулируемой платформы заменяется на заранее подготовленный фрагмент инструкций физического процессора \cite{qemu-ibm}.
Архитектура процесса эмуляции представлена на рис. \ref{emul}.
\addimghere{emulation}{0.35}{Эмуляция аппаратных средств}{emul}

Примерами виртуализации посредством эмуляции являются программные платформы QEMU и Bochs.

Система QEMU поддерживает два режима эмуляции: пользовательский и системный.
Пользовательский режим эмуляции позволяет процессу, созданному на одном процессоре, работать на другом (выполняется динамический перевод инструкций для принимающего процессора и конвертация системных вызовов Linux).
Системный режим позволяет эмулировать систему целиком, включая процессор и разнообразную периферию.
Достоинством QEMU является его быстрый и компактный динамический транслятор.
Динамический транслятор позволяет во время исполнения переводить инструкции целевого (гостевого) процессора в инструкции центрального процессора хоста для обеспечения эмуляции.
QEMU обеспечивает динамическую трансляцию преобразованием целевой инструкции в микрооперации.
Эти микрооперации представляют собой элементы C-кода, которые компилируются в объекты.
Затем запускается основной транслятор, который отображает целевые инструкции на микрооперации для динамической трансляции.
Такой подход не только эффективен, но и обеспечивает переносимость.

Использование QEMU в качестве эмулятора персонального компьютера обеспечивает поддержку разнообразных периферийных устройств.
Сюда входят стандартные периферийные устройства --- эмулятор аппаратного видеоадаптера (VGA), мыши и клавиатуры PS/2, интерфейс IDE для жестких дисков, интерфейс CD-ROM и эмуляция дисковода.
Кроме того, QEMU имеет возможность эмуляции сетевых адаптеров NE2000 (PCI), последовательных портов, многочисленных звуковых плат и контроллера PCI Universal Host Controller Interface (UHCI), Universal Serial Bus (USB) (с виртуальным USB концентратором).
Также поддерживается до 255 процессоров с поддержкой симметричной многопроцессорности (SMP).

Полная виртуализация использует гипервизор, который осуществляет связь между гостевой ОС и аппаратными средствами физического сервера.
В связи с тем, что вся работа с гостевой операционной системой проходит через гипервизор, скорость работы данного типа виртуализации ниже чем в случае прямого взаимодействия с аппаратурой.
Основным преимуществом является то, что в ОС не вносятся никакие изменения, единственное ограничение --- операционная система должна поддерживать основные аппаратные средства.
Архитектура полной виртуализации представлена на рис. \ref{full-virt}.
\addimg{full_virt}{0.35}{Архитектура полной виртуализации}{full-virt}

Полная виртуализация возможна исключительно при условии правильной комбинации оборудования и программного обеспечения.
Например, она была невозможной ни в серии IBM System/360, за исключением IBM System/360-67, ни в ранних IBM System/370, пока IBM не добавила оборудование виртуальной памяти в своих System/370 в 1972 г.
Аналогичная ситуация и с платформой x86: полная виртуализация была возможна не в полной мере, до добавления технологий AMD-V и Intel VT.

KVM (Kernel-based Virtual Machine) --- программное решение, обеспечивающее виртуализацию в среде Linux, которая поддерживает аппаратную виртуализацию на базе Intel VT (Virtualization Technology) либо AMD SVM (Secure Virtual Machine) \cite{kvm-ibm}.
KVM не выполняет никакой самоэмуляции, вместо этого, программа, работающая в пользовательском пространстве, применяет интерфейс /dev/kvm для настройки адресного пространства гостевого виртуального сервера, берет его смоделированные ресурсы ввода/вывода и отображает его образ на образ хоста.

В архитектуре KVM виртуальная машина выполняется как обычный Linux-процесс, запланированный стандартным планировщиком Linux.
На самом деле, виртуальный процессор представляется как обычный Linux-процесс, это позволяет KVM пользоваться всеми возможностями ядра Linux.
Эмуляцией устройств управляет модифицированная версия QEMU, которая обеспечивает эмуляцию BIOS, шины PCI, шины USB, а также стандартный набор устройств таких, как дисковые контроллеры IDE и SCSI, сетевые карты и другие.

Hyper-V является технологией виртуализации на основе гипервизора для 64х-разрядной версии операционной системы Windows Server 2008.
Платформа позволяет несколько изолированным операционным системам разделять одну аппаратную платформу.
Hyper-V оперирует понятием <<раздел>>.
Стек виртуализации работает в родительском разделе и имеет прямой доступ к аппаратным устройствам.
Корневой раздел создает дочерние разделы, в которых располагаются гостевые ОС.
Корневой раздел создает дочерние разделы с помощью интерфейса программирования приложений гипервызовов (API).

Разделы не имеют прямого доступа к физическому процессору и они не могут обрабатывать прерывания.
Вместо этого они имеют доступ к виртуальному процессору и работают в виртуальной памяти.
Гипервизор обрабатывает прерывания процессора и перенаправляет их к соответствующему разделу.
Hyper-V позволяет ускорить трансляцию адресов между различными гостевыми виртуальными адресными пространствами с помощью IOMMU (Input Output Memory Management Unit), который работает независимо от аппаратного управления памятью используемой процессором.

Дочерние разделы также не имеют прямого доступа к аппаратным ресурсам, ресурсы в них представлены в качестве виртуальных устройств (VDevs).
Запросы к виртуальным устройствам перенаправляются либо через VMBus, либо через гипервизор.
Hyper-V требует процессор, которые поддерживает аппаратную виртуализацию (Intel VT или AMD-V).

Паравиртуализация имеет некоторые сходства с полной виртуализацией.
Этот метод использует гипервизор для разделения доступа к основным аппаратным средствам, но объединяет код, касающийся виртуализации, в непосредственно операционную систему, поэтому недостатком метода является то, что гостевая ОС должна быть изменена для гипервизора.
Но паравиртуализация существенно быстрее полной виртуализации, скорость работы виртуальной машины приближена к скорости реальной, это осуществляется за счет отсутствия эмуляции аппаратуры и учета существования гипервизора при выполнении системных вызовов в коде ядра.
Вместо привилегированных операций совершаются гипервызовы обращения ядра гостевой ОС к гипервизору с просьбой о выполнении операции.
Архитектура паравиртуализации представлена на рис. \ref{paravirt}.
\addimg{paravirt}{0.35}{Архитектура паравиртуализации}{paravirt}

Для организации паравиртуализации используется программный продукт Xen.

Xen --- это монитор виртуальных машин (VMM) или гипервизор с поддержкой паравиртуализации для процессоров архитектуры x86, распространяющийся с открытым исходным кодом.
Xen может организовать совместное безопасное исполнение нескольких виртуальных машин на одной физической системе с производительностью, близкой к той, которая была бы непосредственно на физическом оборудовании.
Он перекладывает большинство задач по поддержке аппаратуры на гостевую ОС, работающую в управляющей виртуальной машине, также известной как домен 0 (dom0) \cite{xen-xguru}.
Сам Xen содержит только код, который необходим для обнаружения и запуска остальных процессоров системы, настройки обработки прерываний и нумерации шины PCI.
Драйверы устройств работают не в Xen, а внутри привилегированной гостевой операционной системы.
Такой подход обеспечивает совместимость с большинством устройств, поддерживаемых Linux.
Сборка XenLinux по умолчанию содержит поддержку большого количества серверного сетевого и дискового оборудования, однако при необходимости можно добавить поддержку других устройств, скомпилировав ядро Linux обычным способом.

В паравиртуальном режиме (PV) оборудование не эмулируется, и гостевая операционная система должна быть специальным образом модифицирована для работы в таком окружении.
Начиная с версии 3.0, ядро Linux поддерживает запуск в паравиртуальном режиме без перекомпиляции со сторонними патчами.
Преимущество режима паравиртуализации состоит в том, что он не требует поддержки аппаратной виртуализации со стороны процессора, а также не тратит вычислительные ресурсы для эмуляции оборудования на шине PCI.
Режим аппаратной виртуализации (HVM), который появился в Xen, начиная с версии 3.0 гипервизора требует поддержки со стороны оборудования.
В этом режиме для эмуляции виртуальных устройств используется QEMU, который весьма медлителен несмотря на паравиртуальные драйвера.
Однако со временем поддержка аппаратной виртуализации в оборудовании получила настолько широкое распространение, что используется даже в современных процессорах лэптопов.

Виртуализация уровня операционной системы отличается от других.
Она использует технику, при которой сервера виртуализируются непосредственно над ОС.
Недостатком метода является то, что поддерживается одна единственная операционная система на физическом сервере, которая изолирует контейнеры друг от друга.
Преимуществом виртуализации уровня ОС является <<родная>> производительность.
Виртуализация уровня ОС --- метод виртуализации, при котором ядро операционной системы поддерживает несколько изолированных экземпляров пространства пользователя вместо одного.
Эти экземпляры с точки зрения пользователя полностью идентичны реальному серверу.
Для систем на базе UNIX эта технология может рассматриваться как улучшенная реализация механизма chroot.
Ядро обеспечивает полную изолированность контейнеров, поэтому программы из разных контейнеров не могут воздействовать друг на друга.
Архитектура виртуализации уровня ОС представлена на рис. \ref{cont-virt}.
\addimg{cont_virt}{0.35}{Архитектура виртуализации уровня ОС}{cont-virt}

Для реализации виртуализации уровня операционной системы часто используется продукт OpenVZ.

OpenVZ разрабатывается как патч к исходным текстам ядра Linux.
В модифицированном ядре добавлен массив дополнительных сущностей --- виртуальных окружений (VE) или контейнеров (CT), а для всех имеющихся объектов (процессы, сокеты и прочие) введены дополнительные поля --- номер контейнера, к которому этот объект относится, и номер объекта внутри контейнера.
Каждое виртуальное окружение имеет собственный набор квот на потребление системных ресурсов и отдельный каталог для использования в качестве корневой файловой системы.
Дополнительные модули ядра --- vzdev, vzmon и прочие, отвечают за работу ограничений, мониторинг, эмуляцию сети в контейнере, сохранение и восстановление текущего состояния запущенных контейнеров.
К преимуществам OpenVZ по сравнению с более универсальными инструментами виртуализации, такими, как Xen и KVM, относят прозрачный доступ из внешней системы к процессам, файлам и прочим ресурсам в контейнере.

OpenVZ разрабатывается фирмой Parallels как часть более крупного коммерческого продукта под названием Parallels Virtuozzo Containers (PVC) \cite{lxc-openvz}.
В число преимуществ Virtuozzo по сравнению с OpenVZ входят:
\begin{itemize}
  \item файловая система VZFS;
  \item управление через графическую консоль и веб-интерфейс;
  \item программный интерфейс на базе XML для создания собственных инструментов управления и контроля;
  \item средства миграции с физической системы в контейнер и обратно;
  \item средства контроля за полосой и суммарным потреблением трафика;
  \item интеграция с Plesk, коммерческой панелью управления хостингом;
  \item круглосуточная техническая поддержка.
\end{itemize}

VZFS позволяет совмещать файловые системы контейнеров, при этом базовый образ используется всеми контейнерами, а изменения в нем для каждого контейнера сохраняются раздельно.
Преимущества такого подхода:
\begin{itemize}
  \item место, занимаемое программами на диске, становится фиксированным и не зависит от количества контейнеров, в которые эти программы установлены;
  \item уменьшается расход оперативной памяти, так как код нескольких экземпляров программы или библиотеки, запущенной из одного и того же исполняемого файла, размещается в памяти в единственном экземпляре;
  \item обновление программного обеспечения в группе контейнеров выполняется одной командой.
\end{itemize}

LXC (Linux Containers) --- система виртуализации на уровне операционной системы.
Данная система сходна с OpenVZ и Linux-VServer для Linux, а также FreeBSD jail и Solaris Containers.
LXC основана на технологии cgroups, входящей в ядро Linux, начиная с версии 2.6.29.
Ее нельзя рассматривать как законченный продукт, фактически это набор из нескольких совершенно самостоятельных функций ядра Linux и пользовательских утилит, которые позволяют удобно создавать и управлять изолированными контейнерами \cite{lxc}.
Практически вся функциональность LXC представлена известными механизмами ядра cgroups и namespaces.

CGroups (Control Groups) --- позволяет ограничить аппаратные ресурсы некоторого набора процессов.
Под аппаратными ресурсами подразумеваются: процессорное время, память, дисковая и сетевая подсистемы.
Набор или группа процессов могут быть определены различными критериями.
Например, это может быть целая иерархия процессов, получающая все лимиты родительского процесса.
Кроме этого возможен подсчет расходуемых группой ресурсов, заморозка (freezing) групп, создание контрольных точек (checkpointing) и их перезагрузка.
Для управления этим полезным механизмом существует специальная библиотека libcgroups, в состав которой входят такие утилиты, как cgcreate, cgexec и некоторые другие.

Namespaces --- пространства имен.
Это механизм ядра, который позволяет изолировать процессы друг от друга.
Изоляция может быть выполнена в шести контекстах (пространствах имен):
\begin{itemize}
  \item mount --- предоставляет процессам собственную иерархию файловой системы и изолирует ее от других таких же иерархий по аналогии с chroot;
  \item PID --- изолирует идентификаторы процессов (PID) одного пространства имен от процессов с такими же идентификаторами другого пространства;
  \item network --- предоставляет отдельным процессам логически изолированный от других стек сетевых протоколов, сетевой интерфейс, IP-адрес, таблицу маршрутизации, ARP и прочие реквизиты;
  \item IPC --- обеспечивает разделяемую память и взаимодействие между процессами;
  \item UTS --- изоляция идентификаторов узла, таких как имя хоста (hostname) и домена (domain);
  \item user --- позволяет иметь один и тот же набор пользователей и групп в рамках разных пространств имен, в каждом контейнере могут быть свой root и любые другие пользователи и группы.
\end{itemize}

Одно из главных преимуществ LXC --- это присутствие его базовых блоков (cgroups и namespaces) во всех современных ядрах Linux.
Это означает, что нет необходимости что-то компилировать или использовать стороннее ядро, как в случае с OpenVZ.
Единственное, что необходимо установить, это пакет утилит управления (vzctl, Docker, libvirt, systemd).

К системам управления можно отнести Docker.

Docker --- это программное обеспечение для автоматизации развертывания и управления приложениями в среде виртуализации на уровне операционной системы, например LXC.
Docker позволяет <<упаковать>> приложение и все его окружение и зависимости в контейнер, который легко переносится на любую Linux-систему с поддержкой cgroups в ядре, а также предоставляет среду по управлению контейнерами.

Идея Docker состоит в том, что производитель сам собирает программу со всеми необходимыми зависимостями и поставляет ее в виде контейнера, работающего на платформе Docker, все что останется, это скачать контейнер и запустить его, а впоследствии обновлять.

Для экономии дискового пространства используется файловая система (ФС) Aufs с поддержкой каскадно-объединенного монтирования.
Контейнеры используют образ базовой ОС, а изменения записываются в отдельную область.
Также поддерживается размещение контейнеров в файловой системе Btrfs, в котором включен режим копирования при записи.
В состав Docker входит демон, являющийся сервером контейнеров, клиентские средства, позволяющие из интерфейса командной строки управлять образами и контейнерами, а также программный интерфейс, позволяющий в стиле REST программно управлять контейнерами.
Демон обеспечивает полную изоляцию запускаемых на узле контейнеров на уровне ФС (каждому контейнеру доступна собственная файловая система), на уровне процессов (процессы не имеют доступа к чужой файловой системе контейнера, а ресурсы разделены средствами LXC), на уровне сети (каждому контейнеру привязано сетевое пространства имен и соответствующие виртуальные сетевые интерфейсы).

Набор средств клиента позволяет запускать, приостанавливать и возобновлять процессы в новых контейнерах, останавливать и запускать контейнеры.
Серия команд позволяет осуществлять мониторинг запущенных процессов (по аналогии с ps, top).
Новые образы создаются из специального сценарного файла (dockerfile), в котором возможно записать все изменения, сделанные в контейнере в новый образ.
Все команды могут работать как с docker-демоном локальной системы, так и с сервером docker, доступным по сети.
В интерфейсе командной строки встроены возможности по взаимодействию с публичным репозиторием, именуемым Docker Hub, в котором размещены образы контейнеров, предварительно собранные пользователями, образы можно скачивать в локальную систему, также возможна отправка локально собранных образов в Docker Hub.

Виртуальная инфраструктура является частным облаком, размещаемом на оборудовании провайдера.
Инфраструктура представляет собой динамическое распределение ресурсов в соответствии с нормами предприятия.
Виртуальная машина использует ресурсы одного компьютера, а виртуальная инфраструктура --- всей информационной среды, формируя из компьютеров, а также из подключенных к ним сетей и хранилищ единый пул ресурсов (кластер).

Виртуальная инфраструктура включает в себя следующие компоненты \cite{virt-infrast}:
\begin{itemize}
  \item гипервизоры для одного узла для полной виртуализации каждого компьютера;
  \item пакет услуг инфраструктуры распределенных систем на основе виртуализации (например, управление ресурсами) для оптимального распределения доступных ресурсов между виртуальными машинами;
  \item решения для автоматизации, обеспечивающие особые возможности оптимизации того или иного ИТ-процесса (например инициализации или восстановления в критических ситуациях).
\end{itemize}

Благодаря отделению программной среды от исходной аппаратной инфраструктуры, виртуализация позволяет объединить ряд серверов, инфраструктур хранения данных и сетей в единый пул ресурсов, который динамически, безопасно и надежно распределяется между приложениями по мере необходимости.
С помощью этого решения организации могут создать вычислительную инфраструктуру с максимальной гибкостью, эффективностью, доступностью и автоматизацией, состоящую из недорогих серверов и соответствующих отраслевому стандарту.

Предпочтение виртуальной инфраструктуре отдают по причинам:
\begin{itemize}
  \item экономии на обслуживающем персонале, при условии сохранения полной отказоустойчивости системы;
  \item отсутствии необходимости выделять бюджет на обновление оборудования;
  \item возможности объединения в общую виртуальную среду целые офисы, которые географически находятся в разных местах мира;
  \item возможности быстрого масштабирования проекта;
  \item быстрого доступа к данным.
\end{itemize}

Под каждый тарифный план создается изолированное частное облако с фиксированным количеством выделенных ему ресурсов.
Выделенное частное облако становится гибкой виртуальной оболочкой и функционирующие внутри нее виртуальные машины могут объединяться в виртуальные сети.
Изменение их вычислительной мощности происходит в зависимости от решаемых задач, а смена или преобразование тарифных планов производится в режиме реального времени без перебоев в работе.

Управление ресурсами осуществляется через удобный для пользователя веб-интерфейс, что позволяет делать все необходимые операции с виртуальными машинами, такие как:
\begin{itemize}
  \item создание арендуемых виртуальных серверов самостоятельно;
  \item изменение конфигурации за несколько минут, часть операций даже без остановки и перезагрузки сервера (для некоторых операционных систем);
  \item включение, выключение, установка, переустановка ОС и приложений самостоятельно и удаленно;
  \item осуществление резервного копирование и сохранение состояний (снапшотов) работающих виртуальных машин.
\end{itemize}

\clearpage
